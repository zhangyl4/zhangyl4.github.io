---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# About me
I am Yulin Zhang (Âº†ÂÆáÈ∫ü), a Master's student (2024-2027) at [ShanghaiTech University](https://www.shanghaitech.edu.cn/eng/), advised by [Prof. Sibei Yang](https://sibeiyang.github.io/). Previously, I received my Bachelor's degree from [ShanghaiTech University](https://www.shanghaitech.edu.cn/eng/) in 2024. 

I am honored to work in [SooLab](https://github.com/SooLab) during my master's studies. My research interests include 1. Spatial Intelligence, 2. Multimodal Large Language Models (LLM/MLLM), 3. Open-world Visual Understanding. Currently, I am focusing on equipping multimodal large language models with spatial reasoning abilities, particularly in 3D scene understanding, embodied navigation, and object grounding. Here is my [CV](assets/zyc_cv_en.pdf).

<span style="color:red; font-weight:bold;">**Actively seeking industry research/HC positions and PhD opportunities. I am open to discussing potential collaborations and roles!**</span>

# üî• News
- *2025.09.19*: &nbsp;üéâüéâ Our paper "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video" has been accepted to NeurIPS 2025!
- *2025.07*: &nbsp;üéâüéâ Our paper "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video" has been accepted to ICCV 2025!

# üìù Publications 

\* denotes **equal contribution**

<div class='paper-box'>
  <div class='paper-box-image'>
    <div class="badge">CVPR 2025</div>
    <img src='images/CVPR25_SimCIS.jpg' alt="sym" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">

[Rethinking Query-based Transformer for Continual Image Segmentation](https://arxiv.org/abs/2507.07831)

**Yuchen Zhu**\*, Cheng Shi*, Dingyou Wang, Jiajin Tang, Zhengxuan Wei, Yu Wu, Guanbin Li, Sibei Yang

[[Code]](https://github.com/SooLab/SimCIS)
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div class="badge">ECCV 2024</div>
    <img src='images/ECCV2024_PlainDet.png' alt="sym" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">

[Plain-Det: A Plain Multi-Dataset Object Detector](https://arxiv.org/abs/2407.10083)

Cheng Shi\*, **Yuchen Zhu**\*, Sibei Yang

[[Code]](https://github.com/SooLab/Plain-Det)

  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div class="badge">NeurIPS 2025</div>
    <img src='images/NeurIPS2025_Allpath.png' alt="sym" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">

Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats

Jiaye Qian\*, Ge Zheng\*, **Yuchen Zhu**, Sibei Yang
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div class="badge">ICCV 2025</div>
    <img src='images/ICCV25_SimDetr.png' alt="sym" width="100%">
  </div>
  <div class='paper-box-text' markdown="1">

Sim-DETR: Unlock DETR for Temporal Sentence Grounding

Jiajin Tang\*, Zhengxuan Wei\*, **Yuchen Zhu**, Cheng Shi, Guanbin Li, Liang Lin, Sibei Yang
  </div>
</div>

# ‚ù§Ô∏è Hobbies

<span class='anchor' id='hobbies'></span>

I'm a big ‚öΩ football fan and love playing. I also play guitar for fun‚Äîback in college, I was in a band with friends!


<!-- # üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<br><br><br><br><br>